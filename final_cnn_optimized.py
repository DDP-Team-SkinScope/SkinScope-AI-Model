# -*- coding: utf-8 -*-
"""Final_CNN_Optimized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hAJEGLm8DIpKoWMB_aGxyYiI01fvW_ce
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, models, transforms
import numpy as np
import matplotlib.pyplot as plt
import time
import os
import copy
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from PIL import Image
from skimage import filters
from skimage.measure import shannon_entropy
import cv2
import warnings
warnings.filterwarnings("ignore")

# Check if GPU is available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ========================
# 1. Download and Set Up Datasets
# ========================

# Create main directory for datasets
!mkdir -p /content/skin_datasets

# HAM10000 Dataset
print("\n===== Setting up HAM10000 Dataset =====")
!mkdir -p /content/skin_datasets/ham10000

# Download HAM10000 files
print("Downloading HAM10000 dataset...")
!wget -O /content/skin_datasets/ham10000/ISIC2018_Task3_Training_Input.zip https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task3_Training_Input.zip
!wget -O /content/skin_datasets/ham10000/ISIC2018_Task3_Training_GroundTruth.zip https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task3_Training_GroundTruth.zip
!wget -O /content/skin_datasets/ham10000/ISIC2018_Task3_Training_LesionGroupings.csv https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task3_Training_LesionGroupings.csv

# Extract HAM10000 files
print("Extracting HAM10000 files...")
!unzip -q /content/skin_datasets/ham10000/ISIC2018_Task3_Training_Input.zip -d /content/skin_datasets/ham10000/images
!unzip -q /content/skin_datasets/ham10000/ISIC2018_Task3_Training_GroundTruth.zip -d /content/skin_datasets/ham10000

# ISIC 2019 Dataset
print("\n===== Setting up ISIC 2019 Dataset =====")
!mkdir -p /content/skin_datasets/isic2019

# Download ISIC 2019 files
print("Downloading ISIC 2019 files...")
!wget -O /content/skin_datasets/isic2019/ISIC_2019_Training_Input.zip https://isic-challenge-data.s3.amazonaws.com/2019/ISIC_2019_Training_Input.zip
!wget -O /content/skin_datasets/isic2019/ISIC_2019_Training_Metadata.csv https://isic-challenge-data.s3.amazonaws.com/2019/ISIC_2019_Training_Metadata.csv
!wget -O /content/skin_datasets/isic2019/ISIC_2019_Training_GroundTruth.csv https://isic-challenge-data.s3.amazonaws.com/2019/ISIC_2019_Training_GroundTruth.csv

# Extract ISIC 2019 files
print("Extracting ISIC 2019 files...")
!mkdir -p /content/skin_datasets/isic2019/images
!unzip -q /content/skin_datasets/isic2019/ISIC_2019_Training_Input.zip -d /content/skin_datasets/isic2019/images

from sklearn.model_selection import train_test_split
def build_skin_disease_taxonomy():
    """Build the skin disease taxonomy"""
    # Define taxonomy structure
    taxonomy = {
        "Highly Dangerous": {
            "Malignant Melanoma": ["Superficial spreading melanoma", "Nodular melanoma", "Lentigo maligna melanoma"],
            "Non-Melanoma Skin Cancer": ["Basal cell carcinoma", "Squamous cell carcinoma", "Actinic keratosis"]
        },
        "Minor Conditions": {
            "Benign Lesions": ["Melanocytic nevus", "Dermatofibroma", "Seborrheic keratosis", "Vascular lesions"],
            "Inflammatory Conditions": ["Eczema", "Psoriasis", "Acne", "Rosacea"],
            "Infectious Lesions": ["Fungal infections", "Viral warts", "Bacterial infections"]
        }
    }

    # Create mapping from HAM10000 labels to severity classes
    ham10000_mapping = {
        "MEL": "Highly Dangerous",  # Melanoma
        "BCC": "Highly Dangerous",  # Basal Cell Carcinoma
        "AKIEC": "Highly Dangerous",  # Actinic Keratosis / Intraepithelial Carcinoma
        "SCC": "Highly Dangerous",  # Squamous Cell Carcinoma
        "NV": "Minor Conditions",  # Melanocytic Nevus
        "BKL": "Minor Conditions",  # Benign Keratosis
        "DF": "Minor Conditions",  # Dermatofibroma
        "VASC": "Minor Conditions"  # Vascular Lesions
    }

    return taxonomy, ham10000_mapping

# ========================
# 3. Process and Combine Datasets
# ========================

def process_ham10000():
    """Process HAM10000 dataset and add taxonomy information"""
    # Define paths
    ground_truth_file = "/content/skin_datasets/ham10000/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv"
    lesion_groupings_file = "/content/skin_datasets/ham10000/ISIC2018_Task3_Training_LesionGroupings.csv"

    # Check if files exist
    if not os.path.exists(ground_truth_file):
        print(f"Warning: Ground truth file not found at {ground_truth_file}")
        return pd.DataFrame()

    if not os.path.exists(lesion_groupings_file):
        print(f"Warning: Lesion groupings file not found at {lesion_groupings_file}")
        return pd.DataFrame()

    # Load data files
    ground_truth = pd.read_csv(ground_truth_file)
    lesion_groupings = pd.read_csv(lesion_groupings_file)

    # Get diagnosis columns
    diagnosis_columns = [col for col in ground_truth.columns if col != 'image']

    # Merge datasets and create labels
    merged_data = ground_truth.merge(lesion_groupings, on='image')
    merged_data['label'] = merged_data[diagnosis_columns].idxmax(axis=1)

    # Add image path column
    merged_data['image_path'] = merged_data['image'].apply(
        lambda x: f"/content/skin_datasets/ham10000/images/ISIC2018_Task3_Training_Input/{x}.jpg"
    )

    # Verify that image files exist
    merged_data['image_exists'] = merged_data['image_path'].apply(os.path.exists)
    print(f"HAM10000: {merged_data['image_exists'].sum()}/{len(merged_data)} images found")

    # Keep only existing images
    merged_data = merged_data[merged_data['image_exists']]

    # Add dataset source
    merged_data['dataset_source'] = 'HAM10000'

    # Add severity based on taxonomy
    _, ham10000_mapping = build_skin_disease_taxonomy()
    merged_data['severity'] = merged_data['label'].map(ham10000_mapping)

    return merged_data

def process_isic2019():
    """Process ISIC 2019 dataset and add taxonomy information"""
    # Define paths
    ground_truth_file = "/content/skin_datasets/isic2019/ISIC_2019_Training_GroundTruth.csv"
    metadata_file = "/content/skin_datasets/isic2019/ISIC_2019_Training_Metadata.csv"

    # Check if files exist
    if not os.path.exists(ground_truth_file):
        print(f"Warning: Ground truth file not found at {ground_truth_file}")
        return pd.DataFrame()

    if not os.path.exists(metadata_file):
        print(f"Warning: Metadata file not found at {metadata_file}")
        return pd.DataFrame()

    # Load data files
    ground_truth = pd.read_csv(ground_truth_file)
    metadata = pd.read_csv(metadata_file)

    # Print column names for debugging
    print("Ground truth columns:", ground_truth.columns.tolist())
    print("Metadata columns:", metadata.columns.tolist())

    # Fix column naming inconsistencies
    # ISIC 2019 data might use 'image_name' or 'image' for the image identifier
    if 'image' not in ground_truth.columns and 'image_name' in ground_truth.columns:
        ground_truth.rename(columns={'image_name': 'image'}, inplace=True)

    if 'image' not in metadata.columns and 'image_name' in metadata.columns:
        pass  # Keep as is, we'll handle the merge
    elif 'image_name' not in metadata.columns and 'image' in metadata.columns:
        metadata.rename(columns={'image': 'image_name'}, inplace=True)

    # Identify diagnosis columns (all except image identifier)
    diagnosis_columns = [col for col in ground_truth.columns if col != 'image']

    # Create labels based on maximum probability
    ground_truth['label'] = ground_truth[diagnosis_columns].idxmax(axis=1)

    # Merge with metadata - handle different column names
    if 'image_name' in metadata.columns:
        # If there's an image_name column in metadata, merge on that
        merged_data = ground_truth.merge(metadata, left_on='image', right_on='image_name', how='left')
    else:
        # Otherwise try direct merge
        merged_data = ground_truth.merge(metadata, on='image', how='left')

    # Add image path column
    merged_data['image_path'] = merged_data['image'].apply(
        lambda x: f"/content/skin_datasets/isic2019/images/ISIC_2019_Training_Input/{x}.jpg"
    )

    # Verify that image files exist
    merged_data['image_exists'] = merged_data['image_path'].apply(os.path.exists)
    print(f"ISIC2019: {merged_data['image_exists'].sum()}/{len(merged_data)} images found")

    # Keep only existing images
    merged_data = merged_data[merged_data['image_exists']]

    # Add dataset source
    merged_data['dataset_source'] = 'ISIC2019'

    # Define mapping from ISIC2019 to HAM10000 labels for consistency
    isic2019_mapping = {
        'MEL': 'MEL',
        'NV': 'NV',
        'BCC': 'BCC',
        'AK': 'AKIEC',
        'BKL': 'BKL',
        'DF': 'DF',
        'SCC': 'SCC',
        'VASC': 'VASC'
    }

    # Apply mapping
    merged_data['label'] = merged_data['label'].map(lambda x: isic2019_mapping.get(x, x))

    # Add severity based on taxonomy
    _, ham10000_mapping = build_skin_disease_taxonomy()
    merged_data['severity'] = merged_data['label'].map(ham10000_mapping)

    return merged_data

def combine_datasets():
    """Combine all datasets and create a unified dataframe"""
    print("Processing HAM10000 dataset...")
    ham10000_df = process_ham10000()

    print("Processing ISIC2019 dataset...")
    isic2019_df = process_isic2019()

    # Initialize an empty list to store dataframes
    dfs = []

    # Add dataframes if they're not empty
    if not ham10000_df.empty:
        dfs.append(ham10000_df)
        print(f"HAM10000 dataset: {len(ham10000_df)} images")

    if not isic2019_df.empty:
        dfs.append(isic2019_df)
        print(f"ISIC2019 dataset: {len(isic2019_df)} images")

    # Check if we have any data
    if not dfs:
        raise ValueError("No valid datasets found. Please check paths and data files.")

    # Combine datasets
    print("Combining datasets...")
    combined_df = pd.concat(dfs, ignore_index=True)

    # Drop duplicates based on image path
    combined_df = combined_df.drop_duplicates(subset=['image_path'])

    print(f"Combined dataset created with {len(combined_df)} images")

    # Print class distribution
    print("\nClass distribution by label:")
    label_counts = combined_df['label'].value_counts()
    for label, count in label_counts.items():
        print(f"  {label}: {count} images ({count/len(combined_df)*100:.1f}%)")

    print("\nClass distribution by severity:")
    severity_counts = combined_df['severity'].value_counts()
    for severity, count in severity_counts.items():
        print(f"  {severity}: {count} images ({count/len(combined_df)*100:.1f}%)")

    return combined_df

# ========================
# 4. Denoising and Image Quality Assessment
# ========================

def assess_image_quality(image_path):
    """Assess image quality for filtering low-quality images"""
    try:
        # Read image
        img = cv2.imread(image_path)
        if img is None:
            return False

        # Convert to grayscale
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # Calculate sharpness (Laplacian variance)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()

        # Calculate entropy
        entropy = shannon_entropy(gray)

        # Calculate contrast
        contrast = gray.std()

        # Simple thresholding to separate potential lesion
        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        lesion_area = np.sum(thresh == 255)
        total_area = thresh.size
        lesion_ratio = lesion_area / total_area

        # Define quality criteria
        is_sharp = laplacian_var > 50  # Good sharpness
        has_good_entropy = entropy > 2  # Contains meaningful information
        good_contrast = contrast > 20  # Sufficient contrast
        good_lesion_focus = 0.05 < lesion_ratio < 0.95  # Lesion is visible but not too large or small

        # Consider image good quality if it meets most criteria
        quality_score = sum([is_sharp, has_good_entropy, good_contrast, good_lesion_focus])
        return quality_score >= 3

    except Exception as e:
        print(f"Error assessing image {image_path}: {str(e)}")
        return False

def denoise_dataset(combined_df):
    """Remove low-quality images from the dataset"""
    print("Assessing image quality...")

    # Ensure image_path exists
    if 'image_path' not in combined_df.columns:
        print("Error: 'image_path' column not found in dataframe")
        return combined_df

    # Sample a subset of images for quality assessment to save time
    sample_size = min(1000, len(combined_df))
    sample_df = combined_df.sample(sample_size, random_state=42)

    # Assess quality of sampled images
    quality_results = []
    for idx, row in sample_df.iterrows():
        image_path = row['image_path']
        quality = assess_image_quality(image_path)
        quality_results.append(quality)

    # Calculate quality rate
    quality_rate = sum(quality_results) / len(quality_results)
    print(f"Image quality assessment: {sum(quality_results)}/{len(quality_results)} images passed ({quality_rate:.1%})")

    # If quality is generally good, keep all images
    if quality_rate > 0.9:
        print("Most images have good quality. Keeping all images.")
        return combined_df

    # Otherwise, assess all images and filter
    print("Filtering low-quality images...")
    combined_df['good_quality'] = False  # Initialize column

    # Process in chunks to show progress
    chunk_size = 1000
    total_chunks = len(combined_df) // chunk_size + 1

    for i in range(total_chunks):
        start_idx = i * chunk_size
        end_idx = min((i + 1) * chunk_size, len(combined_df))
        chunk = combined_df.iloc[start_idx:end_idx]

        # Process chunk
        print(f"Processing chunk {i+1}/{total_chunks} ({start_idx} to {end_idx})...")
        for idx, row in chunk.iterrows():
            image_path = row['image_path']
            combined_df.at[idx, 'good_quality'] = assess_image_quality(image_path)

    # Filter dataframe
    filtered_df = combined_df[combined_df['good_quality']]

    print(f"After filtering: {len(filtered_df)}/{len(combined_df)} images kept ({len(filtered_df)/len(combined_df):.1%})")
    return filtered_df

# ========================
# 5. Disease Partitioning and Training Classes
# ========================

def create_training_classes(df, max_classes=7, min_samples=50):
    """Partition diseases into balanced training classes"""
    print("Creating training classes...")

    # Check if df is empty
    if len(df) == 0:
        print("Error: Empty dataframe provided")
        return df

    # Check if required columns exist
    required_cols = ['severity', 'label']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"Error: Missing columns: {missing_cols}")
        return df

    # Start with severity-based classes
    df['training_class'] = df['severity']

    # Get label distribution within each severity class
    high_danger_labels = df[df['severity'] == 'Highly Dangerous']['label'].value_counts()
    minor_labels = df[df['severity'] == 'Minor Conditions']['label'].value_counts()

    # For high danger classes, separate major cancer types
    for label, count in high_danger_labels.items():
        if count >= min_samples:
            # Create a specific class for this cancer type
            mask = (df['label'] == label)
            df.loc[mask, 'training_class'] = f"Dangerous: {label}"

    # For minor conditions, group common conditions
    for label, count in minor_labels.items():
        if count >= min_samples:
            # Create a specific class for this condition
            mask = (df['label'] == label)
            df.loc[mask, 'training_class'] = f"Minor: {label}"

    # Check resulting class distribution
    class_counts = df['training_class'].value_counts()
    print("\nTraining class distribution:")
    for cls, count in class_counts.items():
        print(f"  {cls}: {count} images ({count/len(df)*100:.1f}%)")

    # If we have too many classes, merge smaller ones
    if len(class_counts) > max_classes:
        print(f"\nToo many classes ({len(class_counts)}). Merging smaller classes...")

        # Keep largest classes, merge smaller ones
        top_classes = class_counts.head(max_classes - 1).index.tolist()

        # Update training classes, merging smaller ones
        df['training_class'] = df['training_class'].apply(
            lambda x: x if x in top_classes else "Other conditions"
        )

        # Print updated distribution
        class_counts = df['training_class'].value_counts()
        print("\nRevised training class distribution:")
        for cls, count in class_counts.items():
            print(f"  {cls}: {count} images ({count/len(df)*100:.1f}%)")

    return df

# ========================
# 6. Dataset and DataLoader
# ========================

class SkinLesionDataset(Dataset):
    """Dataset class for skin lesion images"""
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

        # Create class mapping
        self.classes = sorted(dataframe['training_class'].unique())
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}

        # Map training classes to indices
        self.dataframe['class_idx'] = self.dataframe['training_class'].map(self.class_to_idx)

        print(f"Dataset created with {len(self.dataframe)} images and {len(self.classes)} classes")

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_path = self.dataframe.iloc[idx]['image_path']
        class_idx = self.dataframe.iloc[idx]['class_idx']

        # Load image
        try:
            img = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {img_path}: {str(e)}")
            # Create a black dummy image if file can't be read
            img = Image.new('RGB', (224, 224), color='black')

        # Apply transformations
        if self.transform:
            img = self.transform(img)

        return img, class_idx

def create_data_loaders(df, batch_size=32):
    """Create training, validation, and test data loaders"""
    # Check if dataframe is empty or missing required columns
    if len(df) == 0:
        raise ValueError("Empty dataframe provided")

    if 'training_class' not in df.columns:
        raise ValueError("Missing 'training_class' column")

    # Split data into train, validation, and test sets
    train_df, temp_df = train_test_split(
        df, test_size=0.3, random_state=42, stratify=df['training_class']
    )

    val_df, test_df = train_test_split(
        temp_df, test_size=0.5, random_state=42, stratify=temp_df['training_class']
    )

    print(f"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}")

    # Data augmentation for training
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.RandomRotation(20),
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    # Validation and test transforms
    val_test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    # Create datasets
    train_dataset = SkinLesionDataset(train_df, transform=train_transform)
    val_dataset = SkinLesionDataset(val_df, transform=val_test_transform)
    test_dataset = SkinLesionDataset(test_df, transform=val_test_transform)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)

    return train_loader, val_loader, test_loader, train_dataset.classes

# ========================
# 7. CNN Model Architecture (Inception v3)
# ========================

def create_inception_model(num_classes):
    """Create Inception v3 model with custom classifier head"""
    # Load pretrained Inception v3
    model = models.inception_v3(pretrained=True)

    # Modify final classifier
    # Inception v3 has both main output and auxiliary output
    # We need to modify both for fine-tuning

    # Modify main classifier
    in_features = model.fc.in_features
    model.fc = nn.Linear(in_features, num_classes)

    # Modify auxiliary classifier
    if hasattr(model, 'AuxLogits') and hasattr(model.AuxLogits, 'fc'):
        aux_in_features = model.AuxLogits.fc.in_features
        model.AuxLogits.fc = nn.Linear(aux_in_features, num_classes)

    return model

# ========================
# 8. Training Functions
# ========================

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=25):
    """Train the CNN model"""
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    # For tracking training progress
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
                dataloader = train_loader
            else:
                model.eval()   # Set model to evaluate mode
                dataloader = val_loader

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data
            for inputs, labels in dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Zero the parameter gradients
                optimizer.zero_grad()

                # Forward pass
                with torch.set_grad_enabled(phase == 'train'):
                    # Special case for Inception v3 (has auxiliary output)
                    if isinstance(model, models.inception.Inception3) and phase == 'train':
                        outputs, aux_outputs = model(inputs)
                        loss1 = criterion(outputs, labels)
                        loss2 = criterion(aux_outputs, labels)
                        loss = loss1 + 0.4 * loss2
                    else:
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, 1)

                    # Backward pass and optimize only in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # Statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train' and scheduler is not None:
                scheduler.step()

            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_corrects.double() / len(dataloader.dataset)

            # Store history
            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item())
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())

            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # Deep copy the model if best validation accuracy
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_wts)
    return model, history

# ========================
# 9. Evaluation Functions
# ========================

def evaluate_model(model, test_loader, class_names):
    """Evaluate the model on test data"""
    model.eval()

    # Initialize variables for evaluation
    all_preds = []
    all_labels = []
    running_corrects = 0

    # Iterate over test data
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Forward pass
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            # Update statistics
            running_corrects += torch.sum(preds == labels.data)

            # Store predictions and labels for confusion matrix
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate accuracy
    test_acc = running_corrects.double() / len(test_loader.dataset)
    print(f'Test Accuracy: {test_acc:.4f}')

    # Compute confusion matrix
    cm = confusion_matrix(all_labels, all_preds)

    # Plot confusion matrix
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

    return test_acc, cm

def plot_training_history(history):
    """Plot training and validation loss/accuracy"""
    plt.figure(figsize=(12, 5))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history['train_acc'], label='Train')
    plt.plot(history['val_acc'], label='Validation')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history['train_loss'], label='Train')
    plt.plot(history['val_loss'], label='Validation')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# ========================
# 10. Main Execution
# ========================

def main():
    try:
        # 1. Combine datasets
        combined_df = combine_datasets()

        # Make sure we have some data
        if len(combined_df) == 0:
            raise ValueError("No data available after combining datasets")

        # 2. Filter low-quality images
        denoised_df = denoise_dataset(combined_df)

        # 3. Create training classes
        classified_df = create_training_classes(denoised_df)

        # 4. Create data loaders
        train_loader, val_loader, test_loader, class_names = create_data_loaders(classified_df)

        # 5. Create model
        num_classes = len(class_names)
        model = create_inception_model(num_classes)
        model = model.to(device)

        # 6. Set up loss function and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

        # Learning rate scheduler
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

        # 7. Train model
        model, history = train_model(
            model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=15
        )

        # 8. Plot training history
        plot_training_history(history)

        # 9. Evaluate model
        test_acc, cm = evaluate_model(model, test_loader, class_names)

        # 10. Save model
        torch.save(model.state_dict(), '/content/skin_lesion_classifier.pth')
        print("Model saved to '/content/skin_lesion_classifier.pth'")

        return model, class_names

    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None

IMAGE_SIZE = 224  # Changed from 224 to 299 for Inception v3
BATCH_SIZE = 64   # Reduced batch size for stability
NUM_EPOCHS = 10   # Can be reduced for faster debugging
MODEL_TYPE = "resnet50"  # Options: "inception_v3", "resnet50", "efficientnet"
#5. Disease Partitioning and Training Classes
# ========================

def create_training_classes(df, max_classes=7, min_samples=50):
    """Partition diseases into balanced training classes"""
    print("Creating training classes...")

    # Check if df is empty
    if len(df) == 0:
        print("Error: Empty dataframe provided")
        return df

    # Check if required columns exist
    required_cols = ['severity', 'label']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"Error: Missing columns: {missing_cols}")
        return df

    # Start with severity-based classes
    df['training_class'] = df['severity']

    # Get label distribution within each severity class
    high_danger_labels = df[df['severity'] == 'Highly Dangerous']['label'].value_counts()
    minor_labels = df[df['severity'] == 'Minor Conditions']['label'].value_counts()

    # For high danger classes, separate major cancer types
    for label, count in high_danger_labels.items():
        if count >= min_samples:
            # Create a specific class for this cancer type
            mask = (df['label'] == label)
            df.loc[mask, 'training_class'] = f"Dangerous: {label}"

    # For minor conditions, group common conditions
    for label, count in minor_labels.items():
        if count >= min_samples:
            # Create a specific class for this condition
            mask = (df['label'] == label)
            df.loc[mask, 'training_class'] = f"Minor: {label}"

    # Check resulting class distribution
    class_counts = df['training_class'].value_counts()
    print("\nTraining class distribution:")
    for cls, count in class_counts.items():
        print(f"  {cls}: {count} images ({count/len(df)*100:.1f}%)")

    # If we have too many classes, merge smaller ones
    if len(class_counts) > max_classes:
        print(f"\nToo many classes ({len(class_counts)}). Merging smaller classes...")

        # Keep largest classes, merge smaller ones
        top_classes = class_counts.head(max_classes - 1).index.tolist()

        # Update training classes, merging smaller ones
        df['training_class'] = df['training_class'].apply(
            lambda x: x if x in top_classes else "Other conditions"
        )

        # Print updated distribution
        class_counts = df['training_class'].value_counts()
        print("\nRevised training class distribution:")
        for cls, count in class_counts.items():
            print(f"  {cls}: {count} images ({count/len(df)*100:.1f}%)")

    return df

# ========================
# 6. Dataset and DataLoader
# ========================

class SkinLesionDataset(Dataset):
    """Dataset class for skin lesion images"""
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

        # Create class mapping
        self.classes = sorted(dataframe['training_class'].unique())
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}

        # Map training classes to indices
        self.dataframe['class_idx'] = self.dataframe['training_class'].map(self.class_to_idx)

        print(f"Dataset created with {len(self.dataframe)} images and {len(self.classes)} classes")

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_path = self.dataframe.iloc[idx]['image_path']
        class_idx = self.dataframe.iloc[idx]['class_idx']

        # Load image
        try:
            img = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {img_path}: {str(e)}")
            # Create a black dummy image if file can't be read
            img = Image.new('RGB', (224, 224), color='black')

        # Apply transformations
        if self.transform:
            img = self.transform(img)

        return img, class_idx

def get_transforms(image_size):
    """Get train and validation/test transforms with specified image size"""
    train_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.RandomRotation(20),
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    val_test_transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    return train_transform, val_test_transform

def create_data_loaders(df, batch_size=32):
    """Create training, validation, and test data loaders"""
    # Check if dataframe is empty or missing required columns
    if len(df) == 0:
        raise ValueError("Empty dataframe provided")

    if 'training_class' not in df.columns:
        raise ValueError("Missing 'training_class' column")

    # Split data into train, validation, and test sets
    train_df, temp_df = train_test_split(
        df, test_size=0.3, random_state=42, stratify=df['training_class']
    )

    val_df, test_df = train_test_split(
        temp_df, test_size=0.5, random_state=42, stratify=temp_df['training_class']
    )

    print(f"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}")

    # Data augmentation for training
    train_transform = transforms.Compose([
    transforms.Resize((299, 299)),  # Changed from 224x224 to 299x299 for Inception v3
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

    # Validation and test transforms
    val_test_transform = transforms.Compose([
    transforms.Resize((299, 299)),  # Changed from 224x224 to 299x299
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

    # Create datasets
    train_dataset = SkinLesionDataset(train_df, transform=train_transform)
    val_dataset = SkinLesionDataset(val_df, transform=val_test_transform)
    test_dataset = SkinLesionDataset(test_df, transform=val_test_transform)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)

    return train_loader, val_loader, test_loader, train_dataset.classes

def train_with_mixed_precision(model, train_loader, val_loader, criterion,
                              optimizer, scheduler=None, num_epochs=10, device=None):
    """Optimized training with mixed precision and reduced logging"""
    if device is None:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Use mixed precision
    from torch.cuda.amp import autocast, GradScaler
    scaler = GradScaler()

    since = time.time()
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    # Only log every N batches (to reduce overhead)
    log_every = 50

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 20)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
                dataloader = train_loader
            else:
                model.eval()
                dataloader = val_loader

            running_loss = 0.0
            running_corrects = 0

            # Use tqdm for progress bar
            from tqdm import tqdm

            for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=f"{phase}")):
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Zero gradients
                optimizer.zero_grad()

                # Mixed precision forward pass
                with torch.set_grad_enabled(phase == 'train'):
                    with autocast(enabled=device.type=='cuda'):
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, 1)

                    # Backward + optimize only in training phase
                    if phase == 'train':
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()

                # Statistics
                batch_loss = loss.item()
                running_loss += batch_loss * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

                # Print progress occasionally
                if phase == 'train' and (batch_idx + 1) % log_every == 0:
                    samples_so_far = (batch_idx + 1) * inputs.size(0)
                    running_acc = running_corrects.double() / samples_so_far
                    print(f"\nBatch {batch_idx + 1}/{len(dataloader)}: Loss: {batch_loss:.4f}, Acc: {running_acc:.4f}")

            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_corrects.double() / len(dataloader.dataset)

            # Store history
            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item())
                if scheduler is not None:
                    scheduler.step()
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # Save best model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_wts)
    return model, history
# ========================
# 7. CNN Model Architecture (Inception v3)
# ========================

def create_model(model_type, num_classes):
    """Create model based on specified type"""
    if model_type == "inception_v3":
        # Inception v3 requires 299x299 minimum input size
        model = models.inception_v3(pretrained=True)

        # Modify main classifier
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)

        # Modify auxiliary classifier
        if hasattr(model, 'AuxLogits') and hasattr(model.AuxLogits, 'fc'):
            aux_in_features = model.AuxLogits.fc.in_features
            model.AuxLogits.fc = nn.Linear(aux_in_features, num_classes)

    elif model_type == "resnet50":
        # ResNet50 - more robust alternative
        model = models.resnet50(pretrained=True)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)

    elif model_type == "mobilenet_v2":
        # MobileNetV2 - much faster than Inception or ResNet
        model = models.mobilenet_v2(pretrained=True)
        # Replace classifier
        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)


    elif model_type == "efficientnet":
        # EfficientNet - lighter weight option
        model = models.efficientnet_b0(pretrained=True)
        in_features = model.classifier[1].in_features
        model.classifier = nn.Linear(in_features, num_classes)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    return model

# ========================
# 8. Training Functions
# ========================

def train_model(model, train_loader, val_loader, criterion, optimizer,
               scheduler=None, num_epochs=15, device=None, report_interval=10):
    """Train with detailed batch progress reporting"""
    if device is None:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    # For tracking training progress
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': [],
        'batch_losses': [], 'batch_accs': []  # Track individual batch metrics
    }

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 40)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
                dataloader = train_loader
            else:
                model.eval()
                dataloader = val_loader

            running_loss = 0.0
            running_corrects = 0

            # Track batch-level metrics
            batch_losses = []
            batch_accs = []

            # Use tqdm for progress bar
            from tqdm import tqdm
            total_batches = len(dataloader)

            # Iterate over data with progress bar
            for batch_idx, (inputs, labels) in enumerate(tqdm(dataloader, desc=f"{phase.capitalize()}")):
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Zero the parameter gradients
                optimizer.zero_grad()

                # Forward pass
                with torch.set_grad_enabled(phase == 'train'):
                    # Special handling for Inception v3
                    if isinstance(model, models.inception.Inception3) and phase == 'train':
                        try:
                            outputs, aux_outputs = model(inputs)
                            loss1 = criterion(outputs, labels)
                            loss2 = criterion(aux_outputs, labels)
                            loss = loss1 + 0.4 * loss2
                        except RuntimeError:
                            # Fall back to main output only
                            outputs = model(inputs)
                            loss = criterion(outputs, labels)
                    else:
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, 1)

                    # Backward pass and optimize only in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # Calculate batch statistics
                batch_loss = loss.item()
                batch_acc = torch.sum(preds == labels.data).double() / inputs.size(0)

                # Store batch metrics
                batch_losses.append(batch_loss)
                batch_accs.append(batch_acc.item())

                # Update running stats
                running_loss += batch_loss * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

                # Report progress at intervals
                if phase == 'train' and (batch_idx + 1) % report_interval == 0:
                    # Calculate current batch stats
                    print(f"\nBatch {batch_idx + 1}/{total_batches}:")
                    print(f"  Loss: {batch_loss:.4f}")
                    print(f"  Accuracy: {batch_acc:.4f}")

                    # Calculate running stats
                    samples_so_far = (batch_idx + 1) * inputs.size(0)
                    running_loss_avg = running_loss / samples_so_far
                    running_acc_avg = running_corrects.double() / samples_so_far

                    print(f"  Running Loss: {running_loss_avg:.4f}")
                    print(f"  Running Accuracy: {running_acc_avg:.4f}")

                    # Show time elapsed
                    time_elapsed = time.time() - since
                    print(f"  Time: {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s")

            # Calculate epoch metrics
            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_corrects.double() / len(dataloader.dataset)

            # Store history
            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item())
                history['batch_losses'].extend(batch_losses)
                history['batch_accs'].extend(batch_accs)
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())

            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # Deep copy the model if best validation accuracy
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        # Step scheduler
        if scheduler is not None and phase == 'train':
            scheduler.step()

        # Plot current progress after each epoch
        if epoch > 0:  # Need at least 2 epochs for plotting
            plt.figure(figsize=(15, 5))

            # Plot accuracy
            plt.subplot(1, 3, 1)
            plt.plot(history['train_acc'], label='Train')
            plt.plot(history['val_acc'], label='Validation')
            plt.title('Model Accuracy')
            plt.xlabel('Epoch')
            plt.ylabel('Accuracy')
            plt.legend()

            # Plot loss
            plt.subplot(1, 3, 2)
            plt.plot(history['train_loss'], label='Train')
            plt.plot(history['val_loss'], label='Validation')
            plt.title('Model Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.legend()

            # Plot batch metrics (last 100 batches)
            plt.subplot(1, 3, 3)
            batch_indices = list(range(len(history['batch_accs'])))
            plt.plot(batch_indices[-100:], history['batch_accs'][-100:], label='Batch Accuracy')
            plt.title('Recent Batch Accuracy')
            plt.xlabel('Batch')
            plt.ylabel('Accuracy')

            plt.tight_layout()
            plt.show()

        print()

    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:.4f}')

    # Load best model weights
    model.load_state_dict(best_model_wts)
    return model, history

# ========================
# 9. Evaluation Functions
# ========================

def evaluate_model(model, test_loader, class_names):
    """Evaluate the model on test data"""
    model.eval()

    # Initialize variables for evaluation
    all_preds = []
    all_labels = []
    running_corrects = 0

    # Iterate over test data
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Forward pass
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            # Update statistics
            running_corrects += torch.sum(preds == labels.data)

            # Store predictions and labels for confusion matrix
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate accuracy
    test_acc = running_corrects.double() / len(test_loader.dataset)
    print(f'Test Accuracy: {test_acc:.4f}')

    # Compute confusion matrix
    cm = confusion_matrix(all_labels, all_preds)

    # Plot confusion matrix
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

    return test_acc, cm

def plot_training_history(history):
    """Plot training and validation loss/accuracy"""
    plt.figure(figsize=(12, 5))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history['train_acc'], label='Train')
    plt.plot(history['val_acc'], label='Validation')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history['train_loss'], label='Train')
    plt.plot(history['val_loss'], label='Validation')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

combined_df = combine_datasets()
if len(combined_df) == 0:
            raise ValueError("No data available after combining datasets")
denoised_df = denoise_dataset(combined_df)
classified_df = create_training_classes(denoised_df)
classified_df.to_csv('/content/processed_skin_data.csv', index=False)

# Get transforms with the correct image size
train_transform, val_test_transform = get_transforms(IMAGE_SIZE)

# Create data loaders
train_loader, val_loader, test_loader, class_names = create_data_loaders(
    classified_df,
    batch_size=BATCH_SIZE,
)

# Step 3: Create and train model (can be modified and rerun)
num_classes = len(class_names)
model = create_model(MODEL_TYPE, num_classes)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
# Setup optimizer
if MODEL_TYPE == "inception_v3":
    # Lower learning rate for Inception
    optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)
else:
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Learning rate scheduler
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

!nvidia-smi
!pip install torch torchvision tqdm scikit-learn seaborn pandas matplotlib
# Basic imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms

# Data processing and visualization
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# File and system utilities
import os
import time
import copy

# Machine learning utilities
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# Mixed precision training
from torch.cuda.amp import autocast, GradScaler

# Progress bar
from tqdm import tqdm

# Image processing (if needed for denoising)
from PIL import Image
# Define configuration variables
IMAGE_SIZE = 224  # Smaller image size
BATCH_SIZE = 32   # Increase batch size if possible
NUM_EPOCHS = 20   # Fewer epochs
MODEL_TYPE = "mobilenet_v2"  # Lighter model

# Split the classified dataframe into train, validation, and test sets
train_df, temp_df = train_test_split(
    classified_df, test_size=0.3, random_state=42, stratify=classified_df['training_class']
)

val_df, test_df = train_test_split(
    temp_df, test_size=0.5, random_state=42, stratify=temp_df['training_class']
)

print(f"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}")

# Create a lighter model
num_classes = len(class_names)
model = create_model(MODEL_TYPE, num_classes)
model = model.to(device)

# Create simplified transforms
train_transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

val_test_transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Recreate data loaders with faster settings
train_loader = DataLoader(
    SkinLesionDataset(train_df, transform=train_transform),
    batch_size=BATCH_SIZE, shuffle=True,
    num_workers=4, pin_memory=True  # Faster data loading
)

val_loader = DataLoader(
    SkinLesionDataset(val_df, transform=val_test_transform),
    batch_size=BATCH_SIZE, num_workers=4, pin_memory=True
)

test_loader = DataLoader(
    SkinLesionDataset(test_df, transform=val_test_transform),
    batch_size=BATCH_SIZE, num_workers=4, pin_memory=True
)

# Use faster optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train with mixed precision
model, history = train_with_mixed_precision(
    model, train_loader, val_loader, nn.CrossEntropyLoss(),
    optimizer, scheduler=None, num_epochs=NUM_EPOCHS, device=device
)

# Evaluate and save model as before
test_acc, cm = evaluate_model(model, test_loader, class_names)
torch.save(model.state_dict(), '/content/skin_lesion_classifier.pth')
print("Model saved to '/content/skin_lesion_classifier.pth'")